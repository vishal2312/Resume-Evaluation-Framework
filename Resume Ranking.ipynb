{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681e8daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 1_Resume_0.22.pdf saved in /home/vishal/Documents/Python/Ranked-Resumes\n",
      "2. 2_Resume_0.10.pdf saved in /home/vishal/Documents/Python/Ranked-Resumes\n",
      "3. 3_Resume_0.10.pdf saved in /home/vishal/Documents/Python/Ranked-Resumes\n",
      "4. 4_Resume_0.03.pdf saved in /home/vishal/Documents/Python/Ranked-Resumes\n",
      "5. 5_Resume_0.00.pdf saved in /home/vishal/Documents/Python/Ranked-Resumes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # tokenize the text into words\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # stem the words\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    # join the stemmed words back into a single string\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# read the JD\n",
    "jd_filename = \"/home/vishal/Documents/Python/Job-Descriptions/3Pillar_Global-MLE.docx\"\n",
    "jd_text = docx2txt.process(jd_filename)\n",
    "jd_text = preprocess_text(jd_text)\n",
    "\n",
    "# read the resumes from the folder\n",
    "resumes_folder = \"/home/vishal/Documents/Python/Resume\"\n",
    "resumes = []\n",
    "resume_filenames = []\n",
    "for filename in os.listdir(resumes_folder):\n",
    "    if filename.endswith('.pdf'):\n",
    "        with open(os.path.join(resumes_folder, filename), 'rb') as f:\n",
    "            resume_pdf = PyPDF2.PdfReader(f)\n",
    "            resume_text = \"\"\n",
    "            for i in range(len(resume_pdf.pages)):\n",
    "                page = resume_pdf.pages[i]\n",
    "                resume_text += page.extract_text()\n",
    "            resume_text = preprocess_text(resume_text)\n",
    "            resumes.append(resume_text)\n",
    "            resume_filenames.append(filename)\n",
    "\n",
    "# create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# compute the TF-IDF matrix for the JD and the resumes\n",
    "tfidf_matrix = vectorizer.fit_transform([jd_text] + resumes)\n",
    "\n",
    "# compute the cosine similarities between the JD and the resumes\n",
    "cosine_similarities = []\n",
    "for i in range(1, len(resumes)+1):\n",
    "    cosine_similarity = tfidf_matrix[0].dot(tfidf_matrix[i].T).toarray()[0][0]\n",
    "    cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "# rank the resumes based on their cosine similarities to the JD\n",
    "ranked_resumes = sorted(list(enumerate(cosine_similarities)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# create a new folder to save the ranked resumes\n",
    "new_folder_name = \"/home/vishal/Documents/Python/Ranked-Resumes\"\n",
    "os.makedirs(new_folder_name, exist_ok=True)\n",
    "\n",
    "# save the ranked resumes in the new folder\n",
    "for i, (resume_index, similarity) in enumerate(ranked_resumes):\n",
    "    old_filepath = os.path.join(resumes_folder, resume_filenames[resume_index])\n",
    "    new_filename = f\"{i+1}_Resume_{similarity:.2f}.pdf\"\n",
    "    new_filepath = os.path.join(new_folder_name, new_filename)\n",
    "    shutil.copy2(old_filepath, new_filepath)\n",
    "    print(f\"{i+1}. {new_filename} saved in {new_folder_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab6547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222e3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448499d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980faebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "bbdfd9e4",
   "metadata": {},
   "source": [
    "TF = No. of rep of words in sentence / No. of words in sentence\n",
    "IDF = log(No. of sentence / No. of sentence containing words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
